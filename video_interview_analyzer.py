# -*- coding: utf-8 -*-
"""video_interview_analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zbpuBT7w4vE0sjUR4RYMAubIwGEh7g_I
"""

!pip install transformers accelerate bitsandbytes

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.eos_token_id

# Transformers ve peft kütüphaneleri
!pip install transformers accelerate peft datasets

from datasets import load_dataset

dataset = load_dataset("json", data_files="/content/deneme1.jsonl")
dataset = dataset["train"].train_test_split(test_size=0.1)
train_dataset = dataset["train"]
val_dataset = dataset["test"]

def tokenize_fn(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )

train_dataset = train_dataset.map(tokenize_fn, batched=True)
val_dataset = val_dataset.map(tokenize_fn, batched=True)

# PyTorch formatına çevir
train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])
val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

!pip install -q --upgrade transformers datasets accelerate peft

from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model # PEFT için gerekli kütüphaneleri ekle

# LoRA yapılandırması
lora_config = LoraConfig(
    r=8,  # LoRA dikkat katmanları için rank (rank)
    lora_alpha=16,  # LoRA ölçeklendirme faktörü
    target_modules=["q_proj", "v_proj"], # LoRA uygulanacak katmanlar
    lora_dropout=0.1,  # LoRA katmanlarında dropout oranı
    bias="none",  # Bias uygulanmayacak
    task_type="CAUSAL_LM"  # Görev türü: Causal Language Modeling
)

# Modeli PEFT modeli olarak hazırla
model = get_peft_model(model, lora_config)

# Eğitilebilir parametrelerin sayısını yazdır
model.print_trainable_parameters()

training_args = TrainingArguments(
    output_dir="./lora_mistral",
    per_device_train_batch_size=1,  # Colab GPU hafızasına göre
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,   # batch simülasyonu
    learning_rate=3e-4,
    num_train_epochs=3,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    eval_strategy="steps",
    eval_steps=50,
    save_total_limit=2,
    remove_unused_columns=False
)

data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator # Pass the data collator here
)

trainer.train()

def evaluate_answer(question, answer):
    prompt = f"""<s>[INST]
You are a strict senior HR evaluator.

Score the candidate using this rubric:

Clarity (0-2)
Structure (0-2)
Relevance (0-2)
Quality of Example (0-2)
Professional Communication (0-2)

Add the points and give TOTAL score out of 10.

Be strict. Do NOT give high score unless clearly excellent.

Return ONLY in this JSON format:

{{
  "clarity": ,
  "structure": ,
  "relevance": ,
  "example_quality": ,
  "professionalism": ,
  "total_score": ,
  "strengths": "",
  "weaknesses": ""
}}

Question:
{question}

Candidate Answer:
{answer}
[/INST]"""

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

    outputs = model.generate(
        **inputs,
        max_new_tokens=250,
        temperature=0.1
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("[/INST]")[-1].strip()

question = "Tell me about a time you had to learn something quickly."

answer = """
One situation where I had to learn something very quickly was during my internship, when I was asked to fine-tune a large language model for a domain-specific task.

, I had never worked hands-on with parameter-efficient fine-tuning methods like LoRA before. The timeline was tight, and I needed to deliver a working prototype within a short period.

To adapt quickly, I first broke the problem into smaller parts.his experience strengthened my ability to learn under pressure, structure complex topics efficiently, and translate theory into practical implementation quickly."""

print(evaluate_answer(question, answer))

!pip install -q openai-whisper
!apt install -y ffmpeg

import whisper

whisper_model = whisper.load_model("base")  # Changed model to whisper_model
result = whisper_model.transcribe("/content/LLM_Cypher.mp4")

transcript = result["text"]
print(transcript)

question = "Tell me about a time you had to learn something quickly."

answer = transcript

print(evaluate_answer(question, answer))

!pip install opencv-python mediapipe librosa numpy

import cv2
import numpy as np
import librosa

# --- Ayarlar ---
VIDEO_PATH = "/content/LLM_Cypher.mp4"
FPS = 10
DURATION = 5

# --- Haar Cascade Yüz Tespiti ---
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

# --- Video İşleme ---
cap = cv2.VideoCapture(VIDEO_PATH)
video_fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(min(DURATION * video_fps, cap.get(cv2.CAP_PROP_FRAME_COUNT)))
frames_to_process = int(FPS * DURATION)
frame_indices = np.linspace(0, total_frames - 1, frames_to_process).astype(int)

face_detected_frames = 0
mouth_open_values = []

for i in frame_indices:
    cap.set(cv2.CAP_PROP_POS_FRAMES, i)
    ret, frame = cap.read()
    if not ret:
        mouth_open_values.append(0)
        continue

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

    if len(faces) > 0:
        face_detected_frames += 1
        (x, y, w, h) = faces[0]

        # Basit dudak açıklığı tahmini: yüz alt çeyreğini kullan
        mouth_region = gray[y + int(h*0.6): y + h, x: x + w]
        # Dudak açıklığı için yoğunluk farkı
        mouth_open = mouth_region.max() - mouth_region.min()
        mouth_open_values.append(mouth_open)
    else:
        mouth_open_values.append(0)

cap.release()

face_presence_ratio = face_detected_frames / frames_to_process
print("Face Presence Ratio:", face_presence_ratio)

# --- Audio İşleme ---
y, sr = librosa.load(VIDEO_PATH, sr=None, mono=True, offset=0.0, duration=DURATION)
hop_length = max(1, int(len(y) / frames_to_process))
audio_energy = []
for i in range(frames_to_process):
    start = i * hop_length
    end = start + hop_length
    chunk = y[start:end]
    energy = np.sqrt(np.mean(chunk ** 2)) if len(chunk) > 0 else 0
    audio_energy.append(energy)

# --- Lip Sync Skoru ---
correlation = np.corrcoef(audio_energy, mouth_open_values)[0, 1]
print("Lip Sync Correlation:", correlation)

if correlation > 0.6:
    lip_sync_score = 1
elif correlation > 0.3:
    lip_sync_score = 0.5
else:
    lip_sync_score = 0

print("Lip Sync Score:", lip_sync_score)
