{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "QhDVZdShuTP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "2luz76s4uWBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate peft datasets"
      ],
      "metadata": {
        "id": "BjDHc8ocuaVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/deneme1.jsonl\")\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "2WtDX0AeufI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_fn, batched=True)\n",
        "\n",
        "# PyTorch formatına çevir\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
      ],
      "metadata": {
        "id": "dWq38AM6uf_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model # PEFT için gerekli kütüphaneleri ekle\n",
        "\n",
        "# LoRA yapılandırması\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRA dikkat katmanları için rank (rank)\n",
        "    lora_alpha=16,  # LoRA ölçeklendirme faktörü\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # LoRA uygulanacak katmanlar\n",
        "    lora_dropout=0.1,  # LoRA katmanlarında dropout oranı\n",
        "    bias=\"none\",  # Bias uygulanmayacak\n",
        "    task_type=\"CAUSAL_LM\"  # Görev türü: Causal Language Modeling\n",
        ")\n",
        "\n",
        "# Modeli PEFT modeli olarak hazırla\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Eğitilebilir parametrelerin sayısını yazdır\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_mistral\",\n",
        "    per_device_train_batch_size=1,  # Colab GPU hafızasına göre\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,   # batch simülasyonu\n",
        "    learning_rate=3e-4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator # Pass the data collator here\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "NzWNlmWmjbyP",
        "outputId": "2fd5c4a1-b2d4-4976-8edd-30561eea2868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [54/54 06:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.376196</td>\n",
              "      <td>1.544211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=54, training_loss=1.7888555879946109, metrics={'train_runtime': 368.556, 'train_samples_per_second': 0.586, 'train_steps_per_second': 0.147, 'total_flos': 4720554028302336.0, 'train_loss': 1.7888555879946109, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_answer(question, answer):\n",
        "    prompt = f\"\"\"<s>[INST]\n",
        "You are a strict senior HR evaluator.\n",
        "\n",
        "Score the candidate using this rubric:\n",
        "\n",
        "Clarity (0-2)\n",
        "Structure (0-2)\n",
        "Relevance (0-2)\n",
        "Quality of Example (0-2)\n",
        "Professional Communication (0-2)\n",
        "\n",
        "Add the points and give TOTAL score out of 10.\n",
        "\n",
        "Be strict. Do NOT give high score unless clearly excellent.\n",
        "\n",
        "Return ONLY in this JSON format:\n",
        "\n",
        "{{\n",
        "  \"clarity\": ,\n",
        "  \"structure\": ,\n",
        "  \"relevance\": ,\n",
        "  \"example_quality\": ,\n",
        "  \"professionalism\": ,\n",
        "  \"total_score\": ,\n",
        "  \"strengths\": \"\",\n",
        "  \"weaknesses\": \"\"\n",
        "}}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Candidate Answer:\n",
        "{answer}\n",
        "[/INST]\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=250,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"[/INST]\")[-1].strip()"
      ],
      "metadata": {
        "id": "HHET9zkxuvsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Tell me about a time you had to learn something quickly.\"\n",
        "\n",
        "answer = \"\"\"\n",
        "One situation where I had to learn something very quickly was during my internship, when I was asked to fine-tune a large language model for a domain-specific task.\n",
        "\n",
        ", I had never worked hands-on with parameter-efficient fine-tuning methods like LoRA before. The timeline was tight, and I needed to deliver a working prototype within a short period.\n",
        "\n",
        "To adapt quickly, I first broke the problem into smaller parts.his experience strengthened my ability to learn under pressure, structure complex topics efficiently, and translate theory into practical implementation quickly.\"\"\"\n",
        "\n",
        "print(evaluate_answer(question, answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvCrBivFIXVJ",
        "outputId": "87f40595-63e6-459e-9eaf-4b00fa0f07a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"clarity\": 2,\n",
            "  \"structure\": 2,\n",
            "  \"relevance\": 2,\n",
            "  \"example_quality\": 2,\n",
            "  \"professionalism\": 2,\n",
            "  \"total_score\": 10,\n",
            "  \"strengths\": \"Clearly describes a high-pressure learning situation. Demonstrates structured thinking and practical implementation. Mentions a specific technical skill (parameter-efficient fine-tuning).\",\n",
            "  \"weaknesses\": \"Could be more specific about the domain or task. The language model is not named. Could describe the outcome or impact of the prototype.\"\n",
            "}\n",
            "\n",
            "Total Score: 10\n",
            "Strengths: Describes a high-pressure learning situation, structured thinking, practical implementation, specific technical skill.\n",
            "Weaknesses: Could be more specific about the domain or task. The language model is not named. Could describe the outcome or impact of the prototype.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai-whisper\n",
        "!apt install -y ffmpeg"
      ],
      "metadata": {
        "id": "AbdXu9Nbu3X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "whisper_model = whisper.load_model(\"base\")  # Changed model to whisper_model\n",
        "result = whisper_model.transcribe(\"/content/LLM_Cypher.mp4\")\n",
        "\n",
        "transcript = result[\"text\"]\n",
        "print(transcript)"
      ],
      "metadata": {
        "id": "NVbmOR9uu4KJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Tell me about a time you had to learn something quickly.\"\n",
        "\n",
        "answer = transcript\n",
        "\n",
        "print(evaluate_answer(question, answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-xPHD3ZPdL2",
        "outputId": "a779e053-4815-4257-b8e4-5bb668adc3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"clarity\": 0,\n",
            "  \"structure\": 0,\n",
            "  \"relevance\": 0,\n",
            "  \"example_quality\": 0,\n",
            "  \"professionalism\": 0,\n",
            "  \"total_score\": 0,\n",
            "  \"strengths\": \"\",\n",
            "  \"weaknesses\": \"Lacks structure, relevance, and quality. Professionalism is non-existent. No STAR elements. Score: 0\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python mediapipe librosa numpy"
      ],
      "metadata": {
        "id": "F2X2h8jevB-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# --- Ayarlar ---\n",
        "VIDEO_PATH = \"/content/LLM_Cypher.mp4\"\n",
        "FPS = 10\n",
        "DURATION = 5\n",
        "\n",
        "# --- Haar Cascade Yüz Tespiti ---\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# --- Video İşleme ---\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "total_frames = int(min(DURATION * video_fps, cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
        "frames_to_process = int(FPS * DURATION)\n",
        "frame_indices = np.linspace(0, total_frames - 1, frames_to_process).astype(int)\n",
        "\n",
        "face_detected_frames = 0\n",
        "mouth_open_values = []\n",
        "\n",
        "for i in frame_indices:\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        mouth_open_values.append(0)\n",
        "        continue\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        face_detected_frames += 1\n",
        "        (x, y, w, h) = faces[0]\n",
        "\n",
        "        # Basit dudak açıklığı tahmini: yüz alt çeyreğini kullan\n",
        "        mouth_region = gray[y + int(h*0.6): y + h, x: x + w]\n",
        "        # Dudak açıklığı için yoğunluk farkı\n",
        "        mouth_open = mouth_region.max() - mouth_region.min()\n",
        "        mouth_open_values.append(mouth_open)\n",
        "    else:\n",
        "        mouth_open_values.append(0)\n",
        "\n",
        "cap.release()\n",
        "\n",
        "face_presence_ratio = face_detected_frames / frames_to_process\n",
        "print(\"Face Presence Ratio:\", face_presence_ratio)\n",
        "\n",
        "# --- Audio İşleme ---\n",
        "y, sr = librosa.load(VIDEO_PATH, sr=None, mono=True, offset=0.0, duration=DURATION)\n",
        "hop_length = max(1, int(len(y) / frames_to_process))\n",
        "audio_energy = []\n",
        "for i in range(frames_to_process):\n",
        "    start = i * hop_length\n",
        "    end = start + hop_length\n",
        "    chunk = y[start:end]\n",
        "    energy = np.sqrt(np.mean(chunk ** 2)) if len(chunk) > 0 else 0\n",
        "    audio_energy.append(energy)\n",
        "\n",
        "# --- Lip Sync Skoru ---\n",
        "correlation = np.corrcoef(audio_energy, mouth_open_values)[0, 1]\n",
        "print(\"Lip Sync Correlation:\", correlation)\n",
        "\n",
        "if correlation > 0.6:\n",
        "    lip_sync_score = 1\n",
        "elif correlation > 0.3:\n",
        "    lip_sync_score = 0.5\n",
        "else:\n",
        "    lip_sync_score = 0\n",
        "\n",
        "print(\"Lip Sync Score:\", lip_sync_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WXdi2oNgJvL",
        "outputId": "3c3a7ce8-8341-42a3-e502-e22131ff54eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Face Presence Ratio: 0.0\n",
            "Lip Sync Correlation: nan\n",
            "Lip Sync Score: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2068533974.py:51: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(VIDEO_PATH, sr=None, mono=True, offset=0.0, duration=DURATION)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ]
    }
  ]
}
